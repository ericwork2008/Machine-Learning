# Activation Funtions

<figure><img src="../../.gitbook/assets/image (16).png" alt=""><figcaption></figcaption></figure>

## How to choice activition funcion

### Don't use linear activation function in hidden layers

<figure><img src="../../.gitbook/assets/image (20).png" alt=""><figcaption></figcaption></figure>

<figure><img src="../../.gitbook/assets/image (19).png" alt=""><figcaption></figcaption></figure>

ReLU 最常用的原因是它比 sigmoid 的训练速度更快。这是因为 ReLU 只在一边（左边）平坦，而 sigmoid 在曲线两边都平坦（水平，斜率趋近于零）
